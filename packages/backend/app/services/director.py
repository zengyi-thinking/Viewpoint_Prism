"""
Director Service - AI Video Director with Dynamic Audio Narrative
Phase 10: AI Director Expansion

Features:
1. Smart Audio Mixing: Auto-detect when to keep original audio vs AI narration
2. Multi-Persona: Support different narrator personalities (Hajimi, Wukong, Pro)
3. Material-Based: All visuals strictly based on uploaded sources

AI Provider: SophNet (DeepSeek-V3.2 for LLM, CosyVoice for TTS)
"""

import asyncio
import uuid
import subprocess
from pathlib import Path
from typing import Dict, Any, Optional, List
import logging
import json
import re

from app.core import get_settings
from app.services.sophnet_service import get_sophnet_service
from app.services.vector_store import get_vector_store

logger = logging.getLogger(__name__)
settings = get_settings()

# Directory for generated content
GENERATED_DIR = Path(settings.upload_dir).parent / "generated"
GENERATED_DIR.mkdir(parents=True, exist_ok=True)

# Persona configurations (SophNet CosyVoice voice names)
PERSONA_CONFIGS = {
    "hajimi": {
        "name": "å“ˆåŸºç±³",
        "description": "ä½ æ˜¯ä¸€åªå¯çˆ±çš„çŒ«å¨˜è§£è¯´ï¼Œå–œæ¬¢ç”¨'å–µ~'ç»“å°¾ï¼Œè¯­æ°”æ´»æ³¼æ¿€èŒï¼Œç§°å‘¼è§‚ä¼—ä¸º'é“²å±Žå®˜ä»¬'ã€‚",
        "voice": "longxiaochun",  # CosyVoice voice name
        "rate": 1.2,
        "pitch": 1.1,
        "emoji": "ðŸ±",
    },
    "wukong": {
        "name": "å¤§åœ£",
        "description": "ä½ æ˜¯é½å¤©å¤§åœ£å­™æ‚Ÿç©ºï¼Œå–œæ¬¢ç”¨'ä¿ºè€å­™'è‡ªç§°ï¼Œè¯­æ°”ç‹‚å‚²ä¸ç¾ï¼Œç«çœ¼é‡‘ç›ï¼Œå……æ»¡æˆ˜æ–—æ°”æ¯ã€‚",
        "voice": "longxiaochun",
        "rate": 1.1,
        "pitch": 0.9,
        "emoji": "ðŸµ",
    },
    "pro": {
        "name": "ä¸“ä¸šè§£è¯´",
        "description": "ä½ æ˜¯ä¸“ä¸šçš„ç”µç«ž/å‰§æƒ…åˆ†æžå¸ˆï¼Œè¯­æ°”å†·é™å®¢è§‚ï¼Œæ³¨é‡æ•°æ®å’Œé€»è¾‘ï¼Œç”¨è¯ç²¾å‡†ä¸“ä¸šã€‚",
        "voice": "longxiaochun",
        "rate": 1.0,
        "pitch": 1.0,
        "emoji": "ðŸŽ™ï¸",
    },
}


class SequenceSegment:
    """Represents a segment in the director's sequence."""

    def __init__(
        self,
        source: str,  # 'A', 'B', 'intro', 'outro'
        start_hint: float,
        duration: float,
        audio_mode: str,  # 'original' or 'voiceover'
        narration: str = "",
        subtitle: str = "",
        rationale: str = "",
        exact_start: float = None,
    ):
        self.source = source
        self.start_hint = start_hint
        self.duration = duration
        self.audio_mode = audio_mode
        self.narration = narration
        self.subtitle = subtitle
        self.rationale = rationale
        self.exact_start = exact_start if exact_start is not None else start_hint


class DirectorService:
    """AI Director Service for dynamic audio narrative video generation using SophNet."""

    def __init__(self):
        """Initialize with SophNet services."""
        self.sophnet = get_sophnet_service()
        self._tasks: Dict[str, Dict[str, Any]] = {}

    async def generate_narrative_script(
        self,
        conflict_data: Dict[str, Any],
        persona: str,
        asr_data_a: List[Dict[str, Any]] = None,
        asr_data_b: List[Dict[str, Any]] = None,
    ) -> List[SequenceSegment]:
        """
        Generate a narrative script with the Director's brain.

        Args:
            conflict_data: Conflict info with viewpoints
            persona: Persona key ('hajimi', 'wukong', 'pro')
            asr_data_a: ASR data from source A
            asr_data_b: ASR data from source B

        Returns:
            List of SequenceSegment objects representing the video sequence
        """
        persona_config = PERSONA_CONFIGS.get(persona, PERSONA_CONFIGS["pro"])
        view_a = conflict_data.get("viewpoint_a", {})
        view_b = conflict_data.get("viewpoint_b", {})

        # Format ASR context (limit to avoid token overflow)
        asr_context_a = self._format_asr_context(asr_data_a, "A", max_items=10)
        asr_context_b = self._format_asr_context(asr_data_b, "B", max_items=10)

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ‹¥æœ‰ç™¾ä¸‡ç²‰ä¸çš„è§†é¢‘è§£è¯´å¯¼æ¼”ã€‚
å½“å‰äººè®¾æ˜¯ï¼š{persona_config['description']}

åŸºäºŽä¸¤ä¸ªè§†é¢‘çš„å†…å®¹å’Œå†²çªç‚¹ï¼Œç¼–æŽ’ä¸€ä¸ª 60-90ç§’ çš„çŸ­è§†é¢‘è„šæœ¬ã€‚

## å†²çªä¸»é¢˜
{conflict_data.get('topic', 'è§‚ç‚¹å¯¹æ¯”')}

## çº¢æ–¹è§‚ç‚¹ (Source A)
æ ‡é¢˜: {view_a.get('title', 'è§‚ç‚¹A')}
æè¿°: {view_a.get('description', '')}
{asr_context_a}

## è“æ–¹è§‚ç‚¹ (Source B)
æ ‡é¢˜: {view_b.get('title', 'è§‚ç‚¹B')}
æè¿°: {view_b.get('description', '')}
{asr_context_b}

## è¾“å‡ºè¦æ±‚
è¯·è¾“å‡ºä¸€ä¸ª JSON æ•°ç»„ï¼ŒåŒ…å« 6-10 ä¸ªç‰‡æ®µã€‚æ¯ä¸ªç‰‡æ®µåŒ…å«ï¼š
- source: "A" æˆ– "B" æˆ– "intro" (å¼€åœº) æˆ– "outro" (ç»“æŸ)
- start_hint: è§†é¢‘ä¸­çš„å¤§è‡´æ—¶é—´ç‚¹(ç§’æ•°ï¼Œæ•°å­—)
- duration: ç‰‡æ®µæ—¶é•¿(5-10ç§’ï¼Œæ•°å­—)
- audio_mode: "original" (ä¿ç•™åŽŸå£°ç”¨äºŽç²¾å½©æ—¶åˆ») æˆ– "voiceover" (AIè§£è¯´ç”¨äºŽä»‹ç»/è½¬åœº/æ€»ç»“)
- narration: å¦‚æžœæ˜¯ voiceover æ¨¡å¼ï¼Œå†™å‡ºè§£è¯´è¯ï¼ˆå¿…é¡»ç¬¦åˆäººè®¾é£Žæ ¼ï¼‰ï¼›å¦‚æžœæ˜¯ original æ¨¡å¼ï¼Œç•™ç©ºå­—ç¬¦ä¸²
- subtitle: ç”»é¢åº•éƒ¨æ˜¾ç¤ºçš„å­—å¹•å†…å®¹(ç²¾ç®€ç‰ˆï¼Œ10-20å­—)
- rationale: å¯¼æ¼”å¤‡æ³¨ï¼Œä¸ºä»€ä¹ˆé€‰è¿™æ®µ(å†…éƒ¨å‚è€ƒ)

## é‡è¦è§„åˆ™
1. å¼€åœº(intro)ä½¿ç”¨ voiceover ä»‹ç»å†²çª
2. ç²¾å½©ç‰‡æ®µ(å¦‚æ¿€çƒˆå¯¹è¯ã€ååœºé¢)ä½¿ç”¨ original ä¿ç•™åŽŸå£°
3. è½¬åœºå’Œç‚¹è¯„ä½¿ç”¨ voiceover æ·»åŠ è§£è¯´
4. ç»“å°¾(outro)ä½¿ç”¨ voiceover æ€»ç»“
5. è§£è¯´è¯å¿…é¡»ä¸¥æ ¼ç¬¦åˆå½“å‰äººè®¾é£Žæ ¼
6. åªè¾“å‡º JSON æ•°ç»„ï¼Œä¸è¦å…¶ä»–å†…å®¹

è¯·ç›´æŽ¥è¾“å‡º JSON:"""

        try:
            # Use SophNet DeepSeek-V3.2 for script generation
            messages = [
                {"role": "system", "content": f"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†é¢‘å¯¼æ¼”AIã€‚{persona_config['description']}"},
                {"role": "user", "content": prompt}
            ]

            content = await self.sophnet.chat(
                messages=messages,
                model="DeepSeek-V3.2",
                temperature=0.7,
                max_tokens=2000,
            )

            logger.info(f"Director script raw output: {content[:500]}...")

            # Parse JSON from response
            sequence = self._parse_sequence_json(content, persona_config)
            return sequence

        except Exception as e:
            logger.error(f"Director script generation error: {e}")
            # Return fallback sequence
            return self._create_fallback_sequence(conflict_data, persona_config)

    def _format_asr_context(
        self,
        asr_data: List[Dict[str, Any]],
        source_label: str,
        max_items: int = 10
    ) -> str:
        """Format ASR data for context."""
        if not asr_data:
            return f"(Source {source_label} ASRæ•°æ®æš‚ä¸å¯ç”¨)"

        lines = []
        for item in asr_data[:max_items]:
            start = item.get("start", 0)
            text = item.get("text", "")
            if text.strip():
                lines.append(f"[{start:.1f}s] {text}")

        if lines:
            return f"### ASR å†…å®¹ç‰‡æ®µ\n" + "\n".join(lines)
        return f"(Source {source_label} ASRæ•°æ®ä¸ºç©º)"

    def _parse_sequence_json(
        self,
        content: str,
        persona_config: Dict[str, Any]
    ) -> List[SequenceSegment]:
        """Parse JSON sequence from LLM response."""
        # Try to extract JSON array from response
        json_match = re.search(r'\[[\s\S]*\]', content)
        if not json_match:
            logger.warning("No JSON array found in response")
            return []

        try:
            data = json.loads(json_match.group())
            segments = []

            for item in data:
                segment = SequenceSegment(
                    source=item.get("source", "A"),
                    start_hint=float(item.get("start_hint", 0)),
                    duration=float(item.get("duration", 8)),
                    audio_mode=item.get("audio_mode", "voiceover"),
                    narration=item.get("narration", ""),
                    subtitle=item.get("subtitle", ""),
                    rationale=item.get("rationale", ""),
                )
                segments.append(segment)

            logger.info(f"Parsed {len(segments)} segments from director script")
            return segments

        except json.JSONDecodeError as e:
            logger.error(f"JSON parse error: {e}")
            return []

    def _create_fallback_sequence(
        self,
        conflict_data: Dict[str, Any],
        persona_config: Dict[str, Any]
    ) -> List[SequenceSegment]:
        """Create fallback sequence when LLM fails."""
        view_a = conflict_data.get("viewpoint_a", {})
        view_b = conflict_data.get("viewpoint_b", {})
        persona_name = persona_config.get("name", "è§£è¯´å‘˜")

        # Adjust narration style based on persona
        intro_narration = ""
        outro_narration = ""

        if persona_config.get("name") == "å“ˆåŸºç±³":
            intro_narration = f"å–µ~é“²å±Žå®˜ä»¬å¥½å‘€ï¼ä»Šå¤©ç»™å¤§å®¶å¸¦æ¥ä¸€åœºç²¾å½©çš„å¯¹å†³ï¼Œçº¢æ–¹è¯´{view_a.get('title', 'è¿™æ ·')}ï¼Œè“æ–¹è¯´{view_b.get('title', 'é‚£æ ·')}ï¼Œåˆ°åº•è°æ›´åŽ‰å®³å‘¢ï¼Ÿå–µ~"
            outro_narration = f"å¥½å•¦é“²å±Žå®˜ä»¬ï¼Œä»Šå¤©çš„å¯¹å†³å°±åˆ°è¿™é‡Œå–µ~è®°å¾—ç‚¹èµžå…³æ³¨å“¦ï¼å–µå–µ~"
        elif persona_config.get("name") == "å¤§åœ£":
            intro_narration = f"å“¼ï¼ä¿ºè€å­™æ¥ä¹Ÿï¼ä»Šæ—¥è¿™åœºå¯¹å†³ï¼Œçº¢æ–¹ä¸»å¼ {view_a.get('title', 'ç¡¬åˆš')}ï¼Œè“æ–¹åšæŒ{view_b.get('title', 'æ™ºå–')}ï¼Œçœ‹ä¿ºè€å­™ç»™ä½ ä»¬è¯„è¯„ç†ï¼"
            outro_narration = f"å‘”ï¼è¿™åœºå¯¹å†³ï¼Œä¿ºè€å­™çœ‹å¾—åˆ†æ˜Žã€‚è¯¸ä½å¥½è‡ªä¸ºä¹‹ï¼"
        else:
            intro_narration = f"å„ä½è§‚ä¼—å¥½ï¼Œä»Šå¤©æˆ‘ä»¬æ¥åˆ†æžä¸€åœºè§‚ç‚¹å¯¹å†³ã€‚çº¢æ–¹è®¤ä¸º{view_a.get('title', 'è§‚ç‚¹A')}ï¼Œè“æ–¹åˆ™ä¸»å¼ {view_b.get('title', 'è§‚ç‚¹B')}ã€‚è®©æˆ‘ä»¬æ¥çœ‹çœ‹å„è‡ªçš„è®ºæ®ã€‚"
            outro_narration = f"ä»¥ä¸Šå°±æ˜¯æœ¬æœŸçš„å¯¹æ¯”åˆ†æžã€‚ä¸¤ç§è§‚ç‚¹å„æœ‰é“ç†ï¼Œå…³é”®åœ¨äºŽå…·ä½“åœºæ™¯çš„é€‚ç”¨æ€§ã€‚æ„Ÿè°¢è§‚çœ‹ã€‚"

        return [
            SequenceSegment(
                source="intro",
                start_hint=0,
                duration=8,
                audio_mode="voiceover",
                narration=intro_narration,
                subtitle="è§‚ç‚¹å¯¹å†³å¼€å§‹",
                rationale="å¼€åœºä»‹ç»å†²çªèƒŒæ™¯",
            ),
            SequenceSegment(
                source="A",
                start_hint=conflict_data.get("viewpoint_a", {}).get("timestamp", 5) or 5,
                duration=10,
                audio_mode="original",
                narration="",
                subtitle=view_a.get("title", "çº¢æ–¹è§‚ç‚¹"),
                rationale="å±•ç¤ºçº¢æ–¹æ ¸å¿ƒè§‚ç‚¹åŽŸå£°",
            ),
            SequenceSegment(
                source="A",
                start_hint=(conflict_data.get("viewpoint_a", {}).get("timestamp", 5) or 5) + 15,
                duration=8,
                audio_mode="voiceover",
                narration=f"çº¢æ–¹çš„æ ¸å¿ƒè®ºç‚¹æ˜¯{view_a.get('description', '...')[:30]}",
                subtitle="çº¢æ–¹è®ºæ®åˆ†æž",
                rationale="è§£è¯´çº¢æ–¹è®ºæ®",
            ),
            SequenceSegment(
                source="B",
                start_hint=conflict_data.get("viewpoint_b", {}).get("timestamp", 10) or 10,
                duration=10,
                audio_mode="original",
                narration="",
                subtitle=view_b.get("title", "è“æ–¹è§‚ç‚¹"),
                rationale="å±•ç¤ºè“æ–¹æ ¸å¿ƒè§‚ç‚¹åŽŸå£°",
            ),
            SequenceSegment(
                source="B",
                start_hint=(conflict_data.get("viewpoint_b", {}).get("timestamp", 10) or 10) + 15,
                duration=8,
                audio_mode="voiceover",
                narration=f"è“æ–¹çš„æ ¸å¿ƒè®ºç‚¹æ˜¯{view_b.get('description', '...')[:30]}",
                subtitle="è“æ–¹è®ºæ®åˆ†æž",
                rationale="è§£è¯´è“æ–¹è®ºæ®",
            ),
            SequenceSegment(
                source="outro",
                start_hint=0,
                duration=8,
                audio_mode="voiceover",
                narration=outro_narration,
                subtitle="æ„Ÿè°¢è§‚çœ‹",
                rationale="æ€»ç»“æ”¶å°¾",
            ),
        ]

    async def generate_persona_speech(
        self,
        text: str,
        persona: str,
        output_path: Path,
    ) -> bool:
        """
        Generate TTS speech with persona-specific voice settings using SophNet CosyVoice.

        Args:
            text: Narration text
            persona: Persona key
            output_path: Output MP3 path

        Returns:
            True if successful
        """
        persona_config = PERSONA_CONFIGS.get(persona, PERSONA_CONFIGS["pro"])

        try:
            # Use SophNet CosyVoice for TTS
            audio_data = await self.sophnet.generate_speech(
                text=text,
                voice=persona_config["voice"],
                speech_rate=persona_config["rate"],
                pitch_rate=persona_config["pitch"],
            )

            # Write audio data to file
            output_path.parent.mkdir(parents=True, exist_ok=True)
            with open(output_path, "wb") as f:
                f.write(audio_data)

            logger.info(f"Persona speech generated ({persona}): {output_path}")
            return True

        except Exception as e:
            logger.error(f"Persona TTS error ({persona}): {e}")

            # Fallback: generate silent audio
            try:
                duration = max(3, len(text) / 5)  # Estimate duration
                cmd = [
                    "ffmpeg", "-y",
                    "-f", "lavfi",
                    "-i", f"anullsrc=r=44100:cl=stereo",
                    "-t", str(duration),
                    "-c:a", "libmp3lame",
                    "-q:a", "4",
                    str(output_path)
                ]
                result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
                return result.returncode == 0
            except Exception:
                return False

    async def compose_director_cut(
        self,
        sequence: List[SequenceSegment],
        source_a_path: Path,
        source_b_path: Path,
        voiceover_paths: Dict[int, Path],
        output_path: Path,
    ) -> bool:
        """
        Compose director cut video with dynamic audio mixing.

        Args:
            sequence: List of SequenceSegment objects
            source_a_path: Path to video A
            source_b_path: Path to video B
            voiceover_paths: Dict mapping segment index to voiceover MP3 path
            output_path: Output video path

        Returns:
            True if successful
        """
        try:
            output_path.parent.mkdir(parents=True, exist_ok=True)

            # Create temp directory
            temp_dir = GENERATED_DIR / f"temp_director_{uuid.uuid4().hex[:8]}"
            temp_dir.mkdir(parents=True, exist_ok=True)

            processed_clips = []

            for i, segment in enumerate(sequence):
                logger.info(f"Processing segment {i+1}/{len(sequence)}: {segment.source} @ {segment.start_hint}s")

                # Determine source video
                if segment.source in ("intro", "outro"):
                    # Use source A for intro/outro with voiceover
                    source_path = source_a_path
                    start_time = 0 if segment.source == "intro" else max(0, segment.start_hint)
                elif segment.source == "A":
                    source_path = source_a_path
                    start_time = segment.exact_start
                else:  # B
                    source_path = source_b_path
                    start_time = segment.exact_start

                clip_output = temp_dir / f"clip_{i:03d}.mp4"

                # Build FFmpeg command based on audio_mode
                if segment.audio_mode == "original":
                    # Keep original audio at full volume
                    success = await self._process_original_segment(
                        source_path=source_path,
                        start_time=start_time,
                        duration=segment.duration,
                        subtitle=segment.subtitle,
                        output_path=clip_output,
                    )
                else:  # voiceover
                    # Duck original audio and mix with TTS
                    voiceover_path = voiceover_paths.get(i)
                    success = await self._process_voiceover_segment(
                        source_path=source_path,
                        start_time=start_time,
                        duration=segment.duration,
                        subtitle=segment.subtitle,
                        voiceover_path=voiceover_path,
                        output_path=clip_output,
                    )

                if success and clip_output.exists():
                    processed_clips.append(str(clip_output))
                else:
                    logger.warning(f"Segment {i} processing failed, skipping")

            if not processed_clips:
                logger.error("No clips were processed successfully")
                self._cleanup_temp_dir(temp_dir)
                return False

            # Concatenate all clips with crossfade
            final_success = await self._concatenate_with_transition(
                clips=processed_clips,
                output_path=output_path,
                temp_dir=temp_dir,
            )

            # Cleanup
            self._cleanup_temp_dir(temp_dir)

            return final_success

        except Exception as e:
            logger.error(f"Director cut composition error: {e}")
            return False

    async def _process_original_segment(
        self,
        source_path: Path,
        start_time: float,
        duration: float,
        subtitle: str,
        output_path: Path,
    ) -> bool:
        """Process segment keeping original audio."""
        try:
            # Escape subtitle for FFmpeg drawtext
            safe_subtitle = self._escape_ffmpeg_text(subtitle)

            # Build filter: scale to 1280x720, add subtitle overlay
            filter_complex = (
                f"scale=1280:720:force_original_aspect_ratio=decrease,"
                f"pad=1280:720:(ow-iw)/2:(oh-ih)/2:black,"
                f"drawbox=x=0:y=ih-60:w=iw:h=60:color=black@0.6:t=fill,"
                f"drawtext=text='{safe_subtitle}':"
                f"fontsize=28:fontcolor=white:x=(w-text_w)/2:y=h-45:"
                f"borderw=2:bordercolor=black"
            )

            cmd = [
                "ffmpeg", "-y",
                "-ss", str(start_time),
                "-i", str(source_path),
                "-t", str(duration),
                "-vf", filter_complex,
                "-c:v", "libx264",
                "-preset", "fast",
                "-crf", "23",
                "-c:a", "aac",
                "-b:a", "128k",
                str(output_path)
            ]

            result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)

            if result.returncode != 0:
                logger.error(f"Original segment error: {result.stderr[-500:]}")
                return False

            return True

        except Exception as e:
            logger.error(f"Original segment processing error: {e}")
            return False

    async def _process_voiceover_segment(
        self,
        source_path: Path,
        start_time: float,
        duration: float,
        subtitle: str,
        voiceover_path: Optional[Path],
        output_path: Path,
    ) -> bool:
        """Process segment with ducked original audio and voiceover."""
        try:
            safe_subtitle = self._escape_ffmpeg_text(subtitle)

            # Video filter
            video_filter = (
                f"scale=1280:720:force_original_aspect_ratio=decrease,"
                f"pad=1280:720:(ow-iw)/2:(oh-ih)/2:black,"
                f"drawbox=x=0:y=ih-60:w=iw:h=60:color=black@0.6:t=fill,"
                f"drawtext=text='{safe_subtitle}':"
                f"fontsize=28:fontcolor=white:x=(w-text_w)/2:y=h-45:"
                f"borderw=2:bordercolor=black"
            )

            if voiceover_path and voiceover_path.exists():
                # Get voiceover duration to check if we need to extend it
                probe_cmd = [
                    "ffprobe", "-v", "error",
                    "-show_entries", "format=duration",
                    "-of", "default=noprint_wrappers=1:nokey=1",
                    str(voiceover_path)
                ]
                try:
                    vo_result = subprocess.run(probe_cmd, capture_output=True, text=True, timeout=10)
                    vo_duration = float(vo_result.stdout.strip()) if vo_result.stdout.strip() else 0
                except:
                    vo_duration = 0

                # Build audio filter based on voiceover duration
                # If voiceover is shorter than target duration, loop it or extend with silence
                if vo_duration < duration * 0.8:  # If voiceover is significantly shorter
                    # Loop voiceover to match duration, then mix
                    audio_filter = (
                        f"[0:a]atrim=start={start_time}:duration={duration},asetpts=PTS-STARTPTS,volume=0.15[bg];"
                        f"[1:a]aloop=loop=-1:size=2e+09[vo_loop];"
                        f"[vo_loop]atrim=0:{duration},asetpts=PTS-STARTPTS,volume=1.3[vo];"
                        f"[bg][vo]amix=inputs=2:duration=first:dropout_transition=2[audio]"
                    )
                else:
                    # Voiceover is long enough, use normal mix
                    audio_filter = (
                        f"[0:a]atrim=start={start_time}:duration={duration},asetpts=PTS-STARTPTS,volume=0.15[bg];"
                        f"[1:a]atrim=0:{duration},asetpts=PTS-STARTPTS,volume=1.3[vo];"
                        f"[bg][vo]amix=inputs=2:duration=first[audio]"
                    )

                cmd = [
                    "ffmpeg", "-y",
                    "-ss", str(start_time),
                    "-i", str(source_path),
                    "-stream_loop", "-1",  # Loop voiceover input if needed
                    "-i", str(voiceover_path),
                    "-filter_complex", f"[0:v]{video_filter}[video];{audio_filter}",
                    "-map", "[video]",
                    "-map", "[audio]",
                    "-c:v", "libx264",
                    "-preset", "fast",
                    "-crf", "23",
                    "-c:a", "aac",
                    "-b:a", "128k",
                    "-t", str(duration),
                    str(output_path)
                ]
            else:
                # No voiceover available, just use original audio at normal volume
                cmd = [
                    "ffmpeg", "-y",
                    "-ss", str(start_time),
                    "-i", str(source_path),
                    "-t", str(duration),
                    "-vf", video_filter,
                    "-af", "volume=0.3",
                    "-c:v", "libx264",
                    "-preset", "fast",
                    "-crf", "23",
                    "-c:a", "aac",
                    "-b:a", "128k",
                    str(output_path)
                ]

            result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)

            if result.returncode != 0:
                logger.error(f"Voiceover segment error: {result.stderr[-500:]}")
                return False

            return True

        except Exception as e:
            logger.error(f"Voiceover segment processing error: {e}")
            return False

    async def _concatenate_with_transition(
        self,
        clips: List[str],
        output_path: Path,
        temp_dir: Path,
    ) -> bool:
        """Concatenate clips with re-encoding for better compatibility."""
        try:
            # Create concat file
            concat_file = temp_dir / "concat.txt"
            with open(concat_file, "w", encoding="utf-8") as f:
                for clip_path in clips:
                    abs_path = str(Path(clip_path).absolute()).replace(chr(92), '/')
                    f.write(f"file '{abs_path}'\n")

            # Re-encode for better compatibility instead of -c copy
            # This ensures all clips are in the same format
            cmd = [
                "ffmpeg", "-y",
                "-f", "concat",
                "-safe", "0",
                "-i", str(concat_file.absolute()),
                # Video codec settings
                "-c:v", "libx264",
                "-preset", "fast",
                "-crf", "23",
                "-pix_fmt", "yuv420p",  # Ensure compatibility with most players
                "-movflags", "+faststart",  # Enable fast start for web playback
                # Audio codec settings
                "-c:a", "aac",
                "-b:a", "128k",
                str(output_path.absolute())
            ]

            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)

            if result.returncode != 0:
                logger.error(f"Concatenation error: {result.stderr[-500:]}")
                return False

            # Verify output file exists and has content
            if output_path.exists() and output_path.stat().st_size > 1000:
                logger.info(f"Director cut video created: {output_path} ({output_path.stat().st_size} bytes)")
                return True
            else:
                logger.error(f"Output file too small or missing: {output_path}")
                return False

        except subprocess.TimeoutExpired:
            logger.error("Concatenation timeout after 300 seconds")
            return False
        except Exception as e:
            logger.error(f"Concatenation error: {e}")
            return False

    def _escape_ffmpeg_text(self, text: str) -> str:
        """Escape special characters for FFmpeg drawtext filter."""
        if not text:
            return ""
        # Escape special chars: \ ' :
        text = text.replace("\\", "\\\\")
        text = text.replace("'", "\\'")
        text = text.replace(":", "\\:")
        text = text.replace("%", "\\%")
        return text

    def _cleanup_temp_dir(self, temp_dir: Path):
        """Clean up temporary directory."""
        try:
            import shutil
            if temp_dir.exists():
                shutil.rmtree(temp_dir)
        except Exception as e:
            logger.warning(f"Failed to cleanup temp dir: {e}")

    async def get_asr_data(self, source_id: str) -> List[Dict[str, Any]]:
        """Get ASR data for a source from vector store."""
        try:
            vector_store = get_vector_store()
            results = vector_store.search(
                query="",  # Empty query to get all
                n_results=50,
                filter_dict={"source_id": source_id, "chunk_type": "asr"},
            )

            asr_data = []
            for result in results:
                metadata = result.get("metadata", {})
                asr_data.append({
                    "start": metadata.get("start", 0),
                    "end": metadata.get("end", 0),
                    "text": result.get("document", ""),
                })

            # Sort by start time
            asr_data.sort(key=lambda x: x["start"])
            return asr_data

        except Exception as e:
            logger.error(f"Failed to get ASR data: {e}")
            return []

    async def create_director_cut(
        self,
        task_id: str,
        conflict_data: Dict[str, Any],
        source_a_id: str,
        source_a_path: Path,
        time_a: float,
        source_b_id: str,
        source_b_path: Path,
        time_b: float,
        persona: str = "pro",
    ) -> Dict[str, Any]:
        """
        Full pipeline to create a director cut video.

        Args:
            task_id: Unique task identifier
            conflict_data: Conflict info for script generation
            source_a_id: ID of source A
            source_a_path: Path to video A
            time_a: Timestamp for video A
            source_b_id: ID of source B
            source_b_path: Path to video B
            time_b: Timestamp for video B
            persona: Persona key ('hajimi', 'wukong', 'pro')

        Returns:
            Task result with video URL or error
        """
        persona_config = PERSONA_CONFIGS.get(persona, PERSONA_CONFIGS["pro"])

        try:
            # Step 1: Generate narrative script
            self._tasks[task_id] = {
                "status": "generating_script",
                "progress": 10,
                "message": f"ðŸŽ¬ {persona_config['emoji']} å¯¼æ¼”æ­£åœ¨ç¼–å†™å‰§æœ¬...",
            }

            # Get ASR data for context
            asr_data_a = await self.get_asr_data(source_a_id)
            asr_data_b = await self.get_asr_data(source_b_id)

            # Update conflict_data with timestamps
            conflict_data["viewpoint_a"]["timestamp"] = time_a
            conflict_data["viewpoint_b"]["timestamp"] = time_b

            sequence = await self.generate_narrative_script(
                conflict_data=conflict_data,
                persona=persona,
                asr_data_a=asr_data_a,
                asr_data_b=asr_data_b,
            )

            if not sequence:
                raise Exception("å‰§æœ¬ç”Ÿæˆå¤±è´¥")

            # Step 2: Generate voiceovers for voiceover segments
            self._tasks[task_id] = {
                "status": "generating_voiceover",
                "progress": 30,
                "message": f"ðŸŽ¤ {persona_config['name']}æ­£åœ¨å½•åˆ¶è§£è¯´...",
            }

            voiceover_paths = {}
            voiceover_dir = GENERATED_DIR / f"voiceover_{task_id}"
            voiceover_dir.mkdir(parents=True, exist_ok=True)

            for i, segment in enumerate(sequence):
                if segment.audio_mode == "voiceover" and segment.narration:
                    vo_path = voiceover_dir / f"vo_{i:03d}.mp3"
                    success = await self.generate_persona_speech(
                        text=segment.narration,
                        persona=persona,
                        output_path=vo_path,
                    )
                    if success:
                        voiceover_paths[i] = vo_path

            # Step 3: Compose director cut video
            self._tasks[task_id] = {
                "status": "composing_video",
                "progress": 50,
                "message": f"ðŸŽ¬ æ­£åœ¨åŽæœŸåˆæˆ...",
            }

            output_path = GENERATED_DIR / f"director_{task_id}.mp4"
            compose_success = await self.compose_director_cut(
                sequence=sequence,
                source_a_path=source_a_path,
                source_b_path=source_b_path,
                voiceover_paths=voiceover_paths,
                output_path=output_path,
            )

            # Cleanup voiceover files
            self._cleanup_temp_dir(voiceover_dir)

            if not compose_success:
                raise Exception("è§†é¢‘åˆæˆå¤±è´¥")

            # Success
            video_url = f"/static/generated/director_{task_id}.mp4"

            # Build script summary
            script_summary = " â†’ ".join([
                f"{s.source}({'ðŸ”Š' if s.audio_mode == 'original' else 'ðŸŽ¤'})"
                for s in sequence
            ])

            self._tasks[task_id] = {
                "status": "completed",
                "progress": 100,
                "message": f"âœ¨ {persona_config['name']}å¯¼æ¼”ä½œå“å®Œæˆï¼",
                "video_url": video_url,
                "script": script_summary,
                "persona": persona,
                "persona_name": persona_config["name"],
                "segment_count": len(sequence),
            }

            logger.info(f"Director cut video created: {video_url}")
            return self._tasks[task_id]

        except Exception as e:
            logger.error(f"Create director cut error: {e}")
            self._tasks[task_id] = {
                "status": "error",
                "progress": 0,
                "message": f"ç”Ÿæˆå¤±è´¥: {str(e)}",
                "error": str(e),
            }
            return self._tasks[task_id]

    def get_task_status(self, task_id: str) -> Optional[Dict[str, Any]]:
        """Get status of a director cut task."""
        return self._tasks.get(task_id)

    def create_task(self) -> str:
        """Create a new task and return its ID."""
        task_id = str(uuid.uuid4())[:8]
        self._tasks[task_id] = {
            "status": "pending",
            "progress": 0,
            "message": "ç­‰å¾…å¼€å§‹...",
        }
        return task_id


# Singleton instance
_director_service: Optional[DirectorService] = None


def get_director_service() -> DirectorService:
    """Get or create DirectorService singleton."""
    global _director_service
    if _director_service is None:
        _director_service = DirectorService()
    return _director_service
